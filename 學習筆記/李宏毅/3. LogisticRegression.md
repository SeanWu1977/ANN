
```
sigmoid function
Pw,b(C1|x) = σ(z) = σ(w。x + b) = σ(Σwi*xi + b) = 1 / { 1 + exp(-z) } 
z 極大  σ(z) = 1
z 為零  σ(z) = 0.5
z 極小  σ(z) = 0

function set (由很多組w,x所組合而成的fucntion)
fw,b(x) = Pw,b(C1|x) 

how to find best sigmoid function:
假設  
x = [x1, x2, x3] , Y = [1, 0]
(x,Y) => (x1,1),(x2,1),(x3,0)...

Loss Function : 
L(w,b) = fw,b(x1) * fw,b(x2) * (1-fw,b(x3))... 找最大的機率
         ^^^^^^^^              ^^^^^^^^^^^^
      結果為1之機率要越大      因x3是0，所以 fw,b(x3)的機率要小，即 P(1|x3)的機率小              
推導後
training data (X, y) , y = {0,1}
L(f) = Σ C(f(X), y)
C = cross entropy # 當兩個函數f(X), y 越相同，值越小，即代表預測f(X)越接近y
用Gradient Descent方法就可以求最小值。


```

# 比較 Logistic Regression  &  Linear Regression

```
training data (X, y) , 
 
      Logistic Regression  |  Linear Regression
    -------------------------------------------
f   |     σ(Σwi*xi + b)    |     Σwi*xi + b
    |       0  ~  1        |     any value
    
L(f)|     Σ C(f(X), y)     |     Σ(f(X)-y)^2 
    |     y = {0,1}        |       y = ℝ
w   |  w1 = w0 - η*(dL/dw) = w0 - η*Σ-(y-f(x))*x
    |         兩個方法的w更新方法是一樣的(神奇)
    | 把Logistic想成Linear的y只有0,1就可以解釋了 
```

# softmax

```
假設：x的分配是高斯分配，且每個分類的變異數是相同的(同上假設)

C1:w1,b1  z1=w1x+b1
C2:w2,b2  z2=w2x+b2
C3:w3,b3  z3=w3x+b3

softmax
1>y>0
Z = {z1,z2,z3}
y1 = e^z1/Σ(e^z) = p(C1|x)
y2 = e^z2/Σ(e^z) = p(C2|x)
y3 = e^z3/Σ(e^z) = p(C3|x)

將原本x的y值轉為[1,0,0]、[0,1,0]、[0,0,1] (看x值是那一類)
在計算 L(f) = Σ C(f(X), y)
# C = cross entropy # 當兩個函數f(X), y 越相同，值越小，即代表預測f(X)越接近y

```
