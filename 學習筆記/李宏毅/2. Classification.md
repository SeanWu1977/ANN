# Regression for classification : problem
```
用Regression來分類時，為了要讓Loss Function最小，
所以每個點會接近迴歸線。
但如果同一類的點分佈太大，就會造成Loss Function會不準，
因為不能讓遠處的點離迴歸線太遠。
==> Regression 的 Loss Function 的定義照成此問題。

另外，多分類也會是一個問題，會分不出來。
```
# classification
```
重新定義Loss Function : 訓練資料的結果錯誤次數
L(f) = Σδ( f(Xn) != y )
L(f)越小，代表訓練出來的預測越準。
因此方式不可微分，所以不能用regression方法來解。
==> 改用機率(當然還有其它方法，之後會介紹)

問題：假設結果有兩類(Box1,Box2), 給一個值，他是(Box1,Box2)的機率

Box1 = (r,r,r,r,g)
Box2 = (r,r,g,g,g)
P(B1) = 2/3   # 訓練資料可算出
P(B2) = 1/3   # 訓練資料可算出
P(r/B1) = 4/5
P(g/B1) = 1/5
P(r/B2) = 2/5
P(g/B2) = 3/5

if input is r, what does the probability come from B1 ?
P(B1/r) = P(B1) * P(r/B1) / { P(B1) * P(r/B1) + P(B2) * P(r/B2) }
==> r在B1的機率/(r出現在B1 & B2 的機率)

if input is r, what does the probability come from B2 ?
P(B2/r) = P(B2) * P(r/B2) / { P(B1) * P(r/B1) + P(B2) * P(r/B2) }

因分類只有B1 or B2, 所以比較P(B1/r) 與 P(B2/r) 誰大。
分器可以去掉。

Generative Model P(r) = P(B1) * P(r/B1) + P(B2) * P(r/B2) }

```
