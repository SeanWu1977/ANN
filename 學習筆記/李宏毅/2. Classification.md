# Regression for classification : problem
```
用Regression來分類時，為了要讓Loss Function最小，
所以每個點會接近迴歸線。
但如果同一類的點分佈太大，就會造成Loss Function會不準，
因為不能讓遠處的點離迴歸線太遠。
==> Regression 的 Loss Function 的定義照成此問題。

另外，多分類也會是一個問題，會分不出來。
```
# classification
```
重新定義Loss Function : 訓練資料的結果錯誤次數
L(f) = Σδ( f(Xn) != y )
L(f)越小，代表訓練出來的預測越準。
因此方式不可微分，所以不能用regression方法來解。
==> 改用機率(當然還有其它方法，之後會介紹)

問題：假設結果有兩類(Box1,Box2), 給一個值，他是(Box1,Box2)的機率

Box1 = (r,r,r,r,g)
Box2 = (r,r,g,g,g)
P(B1) = 2/3   # 訓練資料可算出
P(B2) = 1/3   # 訓練資料可算出
P(r/B1) = 4/5 # 可利用高斯分配&Maximum Likelihood算出
P(g/B1) = 1/5 # 可利用高斯分配&Maximum Likelihood算出
P(r/B2) = 2/5 # 可利用高斯分配&Maximum Likelihood算出
P(g/B2) = 3/5 # 可利用高斯分配&Maximum Likelihood算出

if input is r, what does the probability come from B1 ?
P(B1/r) = P(B1) * P(r/B1) / { P(B1) * P(r/B1) + P(B2) * P(r/B2) }
==> r在B1的機率/(r出現在B1 & B2 的機率)

if input is r, what does the probability come from B2 ?
P(B2/r) = P(B2) * P(r/B2) / { P(B1) * P(r/B1) + P(B2) * P(r/B2) }

因分類只有B1 or B2, 所以比較P(B1/r) 與 P(B2/r) 誰大。
分器可以去掉。

Generative Model P(r) = P(B1) * P(r/B1) + P(B2) * P(r/B2) }

# 高斯分配&Maximum Likelihood概念
每一個分類中，抽出目前所有點(訓練資料)的最大可能機率。
有一個算式可以根據目前的樣本估出來。即目前樣本的平均值與變異數。
https://en.wikipedia.org/wiki/Maximum_likelihood_estimation

# 百努力分配&Maximum Likelihood
同上，差別在所有的特徵值只有y/n時使用。
白話：x的所有屬性值只有y/n時。

# Naive Bayes Classifier
假設所有的feature(維度，屬性)是獨立：一般是不會獨立，但因為此處是比較誰機率大，所以可以抵消。
=> x = (f1, f2, f3, ... , fn) # x 有 n 個屬性
=> P(x|B1) = P(f1|B1)*P(f2|B1)*P(f3|B1)*...*P(fn|B1)
另外，因為P(x|B2)也用此概念，所以比較P(x|B1) P(x|B2) 就代表特徵間 獨立/相關 的影響可以抵消。

# Posterior Probability
P(C1/r) 
= P(C1) * P(r/C1) / { P(C1) * P(r/C1) + P(C2) * P(r/C2) }
= 1 / { 1 + exp(-z) } 
= σ(z)

z = ln{ ( P(C2)*P(r/C2) ) / ( P(C1)*P(r/C1) )}

σ(z) => sigmoid function
z 極大  σ(z) = 1
z 為零  σ(z) = 0.5
z 極小  σ(z) = 0

# 如果假設兩個分類的變異數是相同的。
上述σ(z)公式就可以推導出以下結果。
σ(z) = σ(w。x + b) = σ(Σwi*xi + b)
然而 w & b 如果要用公式算，有點不方便，
因此就有logistic regression的方法來算w & b, 
而不用公式來算。

PS. 用不同的算法來算，就是"統計"，反正沒有一個標準達案，只有最接近解。

```
