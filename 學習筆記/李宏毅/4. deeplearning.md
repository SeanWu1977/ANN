# neural network 矩陣
```
x1 — n1 —> y1 => active function(ex. sigmod) => new_x1
   X
x2 — n2 —> y2 => active function(ex. sigmod) => new_x2

w1 ==> for x1
w2 ==> for x2

n1 = [n1w1, n1w2]
n2 = [n2w1, n2w2]
n= [n1, n2]
x = [x1, x2]
b = [b1, b2]
n * x + b = y

y1 = n1w1*x1 + n1w2*x2 + b1
y2 = n2w1*x1 + n2w2*x2 + b2

y = f(x) = σ(Wl ... σ((W2*σ(W1x+b1)+b2)..+bl)

計算 Loss (利用cross entropy), 再用Gradient Descent求最小Loss
∇L(w)
```

# backpropagation => grandient descent
```
隨意設定 θ = {w1, w2,..., b1, b2,...}
∇L(θ0) = [∂L(θ0)/∂w1, ∂L(θ0)/∂w2, ..., ∂L(θ0)/∂b1, ∂L(θ0)/∂b2,...]
θ1 = θ0 - η*∇L(θ0)
θ2 = θ1 - η*∇L(θ1)

# Chain Rule
case 1.
y=g(x)
z=h(y)
Δx -> Δy -> Δz (改變x, 就改變y, 相對的也改變z)
dz/dx = (dz/dy) * (dy/dx)

case 2.
x=g(s)
y=h(s)
z=k(x,y)
Δs -> Δx, Δy -> Δz (改變s, 就改變x/y, 相對的也改變z)
dz/ds = (dz/dx) * (dx/ds) + (dz/dy) * (dy/ds) 

# Loss Function
Xn -> NN(θ) -> Yn(預測值) <- Cn(單一輸入的Loss function) -> Y(真實值)
第一輪總所有訓練資料 L(θ) = ΣCn(θ) 
∂L(θ)/∂w = Σ(∂Cn(θ)/∂w)

# backpropagation
x1 — O —> z => σ(z) active function(ex. sigmod)  — O —> z'  .....
   X                                             X                    
x2 — O —> y => σ(y) active function(ex. sigmod)  — O —> y'  .....

z =x1*w1 + x2*w2 + b
∂C/∂w = ∂z/∂w * ∂C/∂z

假設第2層的w 分別為w3, w4 ..
z' = σ(z)w3 + σ(y)w4

- Forward pass : ∂z/∂w 
   ∂z/∂w1 = x1 (規則：即w所接的input值)
   ∂z/∂w2 = x2 (規則：即w所接的input值)
- Backward pass : ∂C/∂z
   a = σ(z)  (σ : active function(ex. sigmod) )
   ∂C/∂z =  ∂a/∂z * ∂C/∂a
   ∂a/∂z = σ'(z) 
   ∂C/∂a = (dz'/da) * (dC/dz') + (dz''/da) * (dC/dz'') 
   (dz'/da)
   依此類堆，直到最後的輸出
   
   
```
