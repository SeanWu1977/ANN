# neural network 矩陣
```
x1 — n1 —> y1 => active function(ex. sigmod) => new_x1
   X
x2 — n2 —> y2 => active function(ex. sigmod) => new_x2

w1 ==> for x1
w2 ==> for x2

n1 = [n1w1, n1w2]
n2 = [n2w1, n2w2]
n= [n1, n2]
x = [x1, x2]
b = [b1, b2]
n * x + b = y

y1 = n1w1*x1 + n1w2*x2 + b1
y2 = n2w1*x1 + n2w2*x2 + b2

y = f(x) = σ(Wl ... σ((W2*σ(W1x+b1)+b2)..+bl)

計算 Loss (利用cross entropy), 再用Gradient Descent求最小Loss
∇L(w)
```

# backpropagation => grandient descent
```
隨意設定 θ = {w1, w2,..., b1, b2,...}
∇L(θ0) = [∂L(θ0)/∂w1, ∂L(θ0)/∂w2, ..., ∂L(θ0)/∂b1, ∂L(θ0)/∂b2,...]
θ1 = θ0 - η*∇L(θ0)
θ2 = θ1 - η*∇L(θ1)

# Chain Rule
case 1.
y=g(x)
z=h(y)
Δx -> Δy -> Δz (改變x, 就改變y, 相對的也改變z)
dz/dx = (dz/dy) * (dy/dx)

case 2.
x=g(s)
y=h(s)
z=k(x,y)
Δs -> Δx, Δy -> Δz (改變s, 就改變x/y, 相對的也改變z)
dz/ds = (dz/dx) * (dx/ds) + (dz/dy) * (dy/ds) 

# Loss Function
Xn -> NN(θ) -> Yn(預測值) <- Cn(單一輸入的Loss function) -> Y(真實值)
第一輪總所有訓練資料 L(θ) = ΣCn(θ) 
∂L(θ)/∂w = Σ(∂Cn(θ)/∂w)

# backpropagation
x1 — O —> z  => σ(z) active function(ex. sigmod)  — O —> z'  .....
   X                                             X                    
x2 — O —> z1 => σ(y) active function(ex. sigmod)  — O —> z1'  .....

z =x1*w1 + x2*w2 + b
∂C/∂w = ∂z/∂w * ∂C/∂z

假設第2層的w 分別為w3, w4 ..
z' = σ(z)w3 + σ(y)w4

- Forward pass : ∂z/∂w 
   ∂z/∂w1 = x1 (規則：即w所接的input值)
   ∂z/∂w2 = x2 (規則：即w所接的input值)
- Backward pass : ∂C/∂z
   a = σ(z)  (σ : active function(ex. sigmod) )
   ∂C/∂z =  ∂a/∂z * ∂C/∂a
   ∂a/∂z = σ'(z) 
   ∂C/∂a = (dz'/da) * (dC/dz') + (dz"/da) * (dC/dz") 
   (dz'/da) = w3
   (dC/dz') => 再用chain rule往後運算即可。
   
   # output layer
      依此類堆，直到最後的輸出 y
      ∂C/∂z' =  ∂y/∂z' * ∂C/∂y 
      (y => active function)
      (C => y^ 與y的距離, cost function, ，ex: cross entropy)
   # not output layer      
      將網路由後往面
      因已知output layer，
      所以可以算出 每個結點的 z"
      也就可以算出σ'(z') 
      原本是                          變為 
      x1 ->  z => σ(z)            a'  <- y1
         X                             X 
      x2                                 y2
                                  因y1, y2可以算出(由output layer)，所以可以推導a'
                                  在用OP AND方式可算出σ'(z'), 因此就可算出∂C/∂z'
 
 總結：
 forward pass => 每個輸入 ∂z/∂w 
 backward pass => 由輸出推導每個 ∂C/∂z
 因此
 ∂C/∂w = ∂z/∂w * ∂C/∂z 
 即是gradiant descent要算的。
 再計算
 ∂L(θ)/∂w = Σ(∂Cn(θ)/∂w)
 後，就可以更新所有的w
 
 
```
